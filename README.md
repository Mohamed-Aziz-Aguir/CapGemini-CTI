Cyber Threat Intelligence Dashboard (CTI)

Authors: Mohamed Aziz Aguir & Yahya Kaddour
Organization / Company: Capgemini (project work)
University: ESPRIT
Supervisor: Mohamed Amine Boussaid
Repository: https://github.com/Mohamed-Aziz-Aguir/CapGemini-CTI

Project start: 2025-06-23 — Project end: today (ongoing)

Project contact: Mohamed Aziz Aguir — mohamedaziz.aguir@outlook.com
 — +216 93 236 576
(Yahya Kaddour contact: please add email/phone in repo CONTRIBUTORS or .env if needed)

Overview

This project is a full-stack Cyber Threat Intelligence (CTI) dashboard consisting of:

Backend: FastAPI (Python) microservice for ingestion, search, threat catalogs, IOC analysis and optional local LLM (Lilly) integration.

Search / Storage: Elasticsearch v8.x for indexing and fast queries.

Cache / PubSub: Redis (optional) for caching and background tasks.

Frontend: Next.js + React UI for browsing threat catalogs, IOC analysis, Zero-Day tracker, news, and Chatbot popup.

Optional AI: Local LLM server based on llama.cpp to host Lilly (Lily-Cybersecurity model) for offline assistant usage.

This README documents setup and how to run everything locally (Ubuntu), full API documentation, dev tips, and references.

Table of contents

Tech stack & Versions

Repository structure

Quick start (one-shot)

Backend — detailed setup & run

Frontend — detailed setup & run

Optional: Local LLM (Lilly) with llama.cpp

API Reference (main endpoints & examples)

Indices & data model (Elasticsearch)

Troubleshooting & common issues

Development notes & tips

Contributors & acknowledgements

License & External resources

Tech stack & Versions

Main runtime and dependencies used in this project:

Python 3.13 (development environment)

FastAPI 0.116.1 — backend web framework

Uvicorn for ASGI HTTP server

Elasticsearch 8.13.0 — primary datastore / search index

Redis 5.0.4 — optional caching/queue

Next.js (React) — frontend (client)

axios, framer-motion, tailwindcss (frontend styling)

llama.cpp & GGUF model (optional local LLM) — Nekuromento/Lily-Cybersecurity-7B-v0.2-Q8_0-GGUF

llama_cpp_python (optional Python bindings) – 0.3.16 (if used in Python)

httpx for HTTP calls in backend.

Pin versions you used in requirements.txt or equivalent for reproducibility.

Repository structure (high level)
/backend
  /app
    /api
      routes/         # FastAPI routers (ioc, zeroday, threat-catalog, lilly, search, news, ...)
    /services         # Business logic (otx_service, virustotal_service, zeroday_service, cve_service, lilly_service)
    /core             # elasticsearch client wrapper, config
    main.py           # FastAPI app
  requirements.txt
  setup_backend_fixed.sh  # one-shot bootstrap script (provided)
  .env               # configuration (not committed)
  docker-compose.yml # created by script or provided
/frontend
  /components
  /pages (or app)    # Next.js pages
  lib/api.ts         # central API client for frontend
  package.json
  tailwind.config.js
README.md            # <-- you are editing this

Quick start (copy-paste friendly)
Option A — Automatic bootstrap (Ubuntu/Debian)

cd backend

Paste & run the provided script (already included in the repo as setup_backend_fixed.sh) or copy-paste the script content and run:

# from backend/ directory
bash setup_backend_fixed.sh
# or optionally with llama.cpp build (no model downloaded)
bash setup_backend_fixed.sh with-lilly


This script will:

Install Docker if missing (Ubuntu)

Create docker-compose.yml with Elasticsearch 8.13.0 and Redis

Start Elasticsearch + Redis

Create Python virtualenv at .venv, install pinned requirements

Create a default .env file (edit for API keys)

Start Uvicorn (FastAPI) on http://0.0.0.0:8000

If you prefer manual steps, see the detailed sections below.

Option B — Manual (recommended if customizing)

Start Elasticsearch + Redis (Docker):

# backend/
cat > docker-compose.yml <<'YAML'
version: "3.8"
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
volumes:
  esdata:
YAML

docker compose up -d


Create Python venv and install:

python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt


Create .env (sample exists or generated by script). Fill API keys (VirusTotal, OTX) if used.

Start backend:

uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload


Start frontend (from /frontend):

npm install
npm run dev
# or
pnpm install
pnpm dev


Frontend default runs on http://localhost:3000.

Backend — detailed setup & run
Environment

Create .env in backend/:

APP_ENV=development
DEBUG=true
HOST=0.0.0.0
PORT=8000

ES_HOST=http://localhost:9200
REDIS_URL=redis://localhost:6379/0

VT_API_KEY=
OTX_API_KEY=

LILLY_SERVER_URL=http://localhost:8080


Make sure Elasticsearch is healthy at http://localhost:9200.

Elasticsearch client

Your backend uses an Async Elasticsearch client cached in app/core/elasticsearch_client.py. Ensure correct host / auth values pulled from .env.

Routes (examples)

/api/ioc/analyze — POST for IOC analysis

/zeroday/search — GET for Zero-Day search (query param query)

/threat-catalog/get — GET for threat catalog (query param category)

/api/lilly/chat — Chat endpoint to Lilly (streams)

/api/search/all — CVE search/browse

See API docs: http://localhost:8000/docs

Services

zeroday_service.py — searches zeroday index, returns hits

cve_service.py — CVE search with pagination

threat_catalog_service.py — fetch threat category indices

lilly_service.py — robust streaming/backfill for the local LLM backend (llama server)

Frontend — setup & run

The frontend is a Next.js app with pages:

/ioc — IOC analyzer page

/threat-catalog — browse threat categories

/zeroday — zero-day tracker

Chatbot popup uses API /api/lilly/chat

API client

/lib/api.ts contains axios-based functions and uses NEXT_PUBLIC_API_BASE_URL environment variable or defaults to http://localhost:8000.

Example:

const API_BASE = process.env.NEXT_PUBLIC_API_BASE_URL || "http://localhost:8000";

export async function getThreatCatalog(category: string) {
  const res = await axios.get(`${API_BASE}/threat-catalog/get`, { params: { category } });
  return res.data;
}

export async function searchZeroDay(q: string) {
  const res = await axios.get(`${API_BASE}/zeroday/search`, { params: { query: q } });
  return res.data;
}


Ensure frontend calls use param names expected by the backend (e.g., query, category, etc.)

Run

From /frontend:

npm install
npm run dev
# or with pnpm/yarn accordingly


Open http://localhost:3000.

Optional: Local LLM (Lilly) with llama.cpp

Note: This is optional. Local LLM requires a large model download (GGUF) and llama.cpp compiled binaries.

Steps (high-level):

Clone & build llama.cpp:

git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
# optional compile options (LLAMA_CUDA=1 for CUDA, LLAMA_CURL=1):
LLAMA_CURL=1 make
# on machines without GPUs, default build is fine


Download the GGUF model from Hugging Face:

Model used: Nekuromento/Lily-Cybersecurity-7B-v0.2-Q8_0-GGUF
You must have the right to download the model from Hugging Face and place it in backend models folder:

mkdir -p ~/CTI/backend/models/lilly
# Download manually via browser or HF CLI and move here:
# Example (if using HF CLI with credentials):
# hf_hub_download --repo_id Nekuromento/Lily-Cybersecurity-7B-v0.2-Q8_0-GGUF --filename lily-cybersecurity-7b-v0.2-q8_0.gguf --repo_type=model
mv lily-cybersecurity-7b-v0.2-q8_0.gguf ~/CTI/backend/models/lilly/


Start llama-server:

cd ~/CTI/backend/models/llama.cpp/build/bin
./llama-server \
  -m ~/CTI/backend/models/lilly/lily-cybersecurity-7b-v0.2-q8_0.gguf \
  --host 0.0.0.0 \
  --port 8080 \
  --ctx-size 4096 \
  --n-gpu-layers 0


Configure backend .env LILLY_SERVER_URL=http://localhost:8080 and restart backend.

Notes & credits

llama.cpp repo: https://github.com/ggerganov/llama.cpp

Model: https://huggingface.co/Nekuromento/Lily-Cybersecurity-7B-v0.2-Q8_0-GGUF

Make sure you comply with the model license and Hugging Face terms before downloading and using.

API Reference (select endpoints & examples)
/zeroday/search (GET)

Search zero-day index. Param: query (optional). If omitted or empty, returns all results (or first N).

Example:

GET /zeroday/search?query=electric


Response:

{
  "count": 96,
  "results": [
    {"zero_day_id":"ZDI-CAN-26359","cve":"Not yet assigned","category":"Electric Vehicle Chargers","impact":"Bypass authentication on the system."},
    ...
  ]
}

/threat-catalog/get (GET)

Get entire threat catalog for given category index:

GET /threat-catalog/get?category=execution


Returns list of top-level threat docs (each may contain SubThreats array).

/api/ioc/analyze (POST)

Analyze an IOC using OTX and VirusTotal services. Body:

{ "value": "github.com" }

/api/lilly/chat (POST)

Chat with Lilly (local LLM). Streaming and non-streaming modes supported. See app/services/lilly_service.py for streaming logic.

Indices & data model (Elasticsearch)

The backend expects certain indices (names exactly as in ES):

asrg-cve (CVE database)

zeroday

otx-iocs

vt-iocs

Threat catalog indices:

execution

privilege_escalation

lateral_movement

initial_access

collection

command_and_control

defense_ecasion (typo: index name intentionally kept to match your ES)

credential_access

discovery

persistence

tampering

exfiltration

spoofing

information_disclosure

repudiation

manipulate_environment

Each threat doc typically contains top-level fields like ThreatName, ThreatID, and SubThreats (an array of sub-threat entries with ThreatName, ThreatID, AttackFeasibilityLevel, FeasibilityRating, Description, SecurityProperties).

Troubleshooting & common issues

404 from /zeroday/search: Ensure backend route is mounted under /zeroday and frontend calls use ?query= param name exactly. In lib/api.ts, searchZeroDay() should call params: { query: q }.

Empty frontend results: Confirm backend returns {"count": N, "results": [...]} and frontend uses response.data.results.

Elasticsearch not found / 500 errors: Check Docker container status docker compose ps and logs docker compose -f docker-compose.yml logs elasticsearch.

CORS: Backend includes CORS middleware; ensure frontend base URL matches NEXT_PUBLIC_API_BASE_URL or uses defaults.

Lilly streaming issues: If streaming yields funky characters or spacing, use the refined token-cleaning logic in app/services/lilly_service.py (already included). If llama-server binary fails, ensure the GGUF model path is correct and the server supports SSE / chunked streaming.

Docker permission error: If your user added to docker group, log out and back in (or restart the shell).

Development notes & tips

Use the included setup_backend_fixed.sh for reproducible environment on Ubuntu.

Keep .env out of version control — store API keys using GitHub Secrets in production.

For heavy indexing of CVEs and Zero-Day data, prepare bulk index scripts to push JSON to ES (via _bulk API).

Use Kibana or Elastic's Dev Tools when debugging indices (optional).

For frontend structure, keep lib/api.ts as a single source of truth for backend endpoints. That makes swapping base URLs for staging easier.

Contributors & acknowledgements

Mohamed Aziz Aguir — main dev & contact (mohamedaziz.aguir@outlook.com
)

Yahya Kaddour — co-author

Supervisor: Mohamed Amine Boussaid

Work performed at Capgemini as a project (study at ESPRIT)

Third-party projects & references

FastAPI — https://fastapi.tiangolo.com/

Uvicorn — https://www.uvicorn.org/

Elasticsearch — https://www.elastic.co/

Redis — https://redis.io/

llama.cpp — https://github.com/ggerganov/llama.cpp

Model: Nekuromento/Lily-Cybersecurity-7B-v0.2-Q8_0-GGUF — https://huggingface.co/Nekuromento/Lily-Cybersecurity-7B-v0.2-Q8_0-GGUF

Framer Motion — https://www.framer.com/motion/

Next.js — https://nextjs.org/

Please consult each upstream project's license before distribution.

License

Add your chosen license file (e.g., LICENSE MIT or your org's license). This repo does not ship third-party model files — users must download GGUF models per the owner’s license.

Example: Common commands summary

Start ES & Redis:

# from backend/
docker compose -f docker-compose.yml up -d


Install backend deps (manual):

python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000


Start frontend:

# from frontend/
npm install
npm run dev


Build and run llama server (optional):

# build llama.cpp first (see instructions)
cd ~/CTI/backend/models/llama.cpp/build/bin
./llama-server -m ~/CTI/backend/models/lilly/lily-cybersecurity-7b-v0.2-q8_0.gguf --host 0.0.0.0 --port 8080 --ctx-size 4096 --n-gpu-layers 0

Diagrams (Mermaid supported on GitHub)

You can render quick architecture diagrams on GitHub using Mermaid. Add this to markdown:

graph TD
  A[Frontend (Next.js)] -->|REST / Axios| B(Backend: FastAPI)
  B -->|Queries| C[Elasticsearch 8.13]
  B -->|Cache| D[Redis]
  B -->|External APIs| E[OTX / VirusTotal]
  B -->|Optional| F[Lilly LLM (llama-server)]
  F -->|uses| G[GGUF Model (Lily-Cybersecurity)]

Final notes

This README aims to be a complete guide to set up, run, and extend your CTI project.

Add any organization-specific secrets and credentials into .env or use environment-specific deployment procedure for production.

If you want, I can also:

generate a polished CONTRIBUTING.md, CODE_OF_CONDUCT.md, or LICENSE.

convert this README to a multi-file docs/ site.

create a short bash installer for frontend (Next.js) as well.
